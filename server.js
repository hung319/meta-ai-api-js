// server.js
require('dotenv').config();
const express = require('express');
const bodyParser = require('body-parser');
const cors = require('cors');
const { v4: uuidv4 } = require('uuid');
const MetaAI = require('./index'); // Import tá»« source code gá»‘c

const app = express();
const PORT = process.env.PORT || 3000;

// Cáº¤U HÃŒNH API KEY (Máº·c Ä‘á»‹nh lÃ  "1" náº¿u khÃ´ng cÃ³ env)
const SERVER_API_KEY = process.env.API_KEY || "1";
const PROXY_URL = process.env.PROXY_URL || null;

// --- GLOBAL INSTANCE MANAGER ---
// Biáº¿n nÃ y giá»¯ káº¿t ná»‘i Ä‘á»ƒ khÃ´ng pháº£i login láº¡i má»—i request
let metaInstance = null;

async function getMetaAIInstance() {
    if (!metaInstance) {
        console.log('ğŸ”„ Initializing new MetaAI instance...');
        try {
            // Khá»Ÿi táº¡o MetaAI (cÃ³ thá»ƒ truyá»n email/pass vÃ o Ä‘Ã¢y náº¿u muá»‘n login Facebook)
            // VÃ­ dá»¥: await MetaAI.create(process.env.FB_EMAIL, process.env.FB_PASS, PROXY_URL);
            metaInstance = await MetaAI.create(null, null, PROXY_URL);
            console.log('âœ… MetaAI instance initialized.');
        } catch (error) {
            console.error('âŒ Failed to initialize MetaAI:', error);
            throw error;
        }
    }
    return metaInstance;
}

// --- MIDDLEWARES ---
app.use(cors());
app.use(bodyParser.json());

// Auth Middleware
app.use((req, res, next) => {
    // Bá» qua check auth cho health check hoáº·c root
    if (req.path === '/') return next();

    // Láº¥y token tá»« header Authorization
    const authHeader = req.headers.authorization;
    const token = authHeader && authHeader.split(' ')[1]; // Format: "Bearer <token>"

    if (token !== SERVER_API_KEY) {
        return res.status(401).json({
            error: {
                message: "Invalid API Key",
                type: "invalid_request_error",
                param: null,
                code: "invalid_api_key"
            }
        });
    }
    next();
});

// --- HELPER FUNCTIONS ---

function convertMessagesToPrompt(messages) {
    // Láº¥y tin nháº¯n cuá»‘i cÃ¹ng cá»§a User Ä‘á»ƒ gá»­i cho Meta AI
    // (LÃ½ do: MetaAI instance tá»± lÆ°u context há»™i thoáº¡i bÃªn trong nÃ³ rá»“i)
    const lastUserMessage = messages.slice().reverse().find(m => m.role === 'user');
    return lastUserMessage ? lastUserMessage.content : "Hello";
}

// --- ENDPOINTS ---

// Health check
app.get('/', (req, res) => {
    res.send('Meta AI OpenAI Wrapper is running. Use endpoint /v1/chat/completions');
});

// 1. List Models Endpoint
app.get('/v1/models', (req, res) => {
    res.json({
        object: "list",
        data: [
            {
                id: "meta-llama-3",
                object: "model",
                created: 1677610602,
                owned_by: "meta-ai-wrapper",
            },
            {
                id: "gpt-3.5-turbo", // Alias cho tÆ°Æ¡ng thÃ­ch client cÅ©
                object: "model",
                created: 1677610602,
                owned_by: "meta-ai-wrapper"
            }
        ]
    });
});

// 2. Chat Completions Endpoint
app.post('/v1/chat/completions', async (req, res) => {
    const { messages, stream = false, model } = req.body;

    if (!messages || !Array.isArray(messages)) {
        return res.status(400).json({ error: "Messages array is required" });
    }

    try {
        const meta = await getMetaAIInstance();
        const prompt = convertMessagesToPrompt(messages);
        
        // Máº·c Ä‘á»‹nh false Ä‘á»ƒ giá»¯ context há»™i thoáº¡i. 
        // Náº¿u muá»‘n reset, client cÃ³ thá»ƒ gá»­i param riÃªng (nhÆ°ng API OpenAI chuáº©n khÃ´ng cÃ³ param nÃ y)
        const isNewConversation = false; 

        if (!stream) {
            // --- NON-STREAMING RESPONSE ---
            const response = await meta.prompt(prompt, false, isNewConversation);
            
            res.json({
                id: `chatcmpl-${uuidv4()}`,
                object: "chat.completion",
                created: Math.floor(Date.now() / 1000),
                model: model || "meta-llama-3",
                choices: [{
                    index: 0,
                    message: {
                        role: "assistant",
                        content: response.message,
                    },
                    finish_reason: "stop"
                }],
                usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }
            });

        } else {
            // --- STREAMING RESPONSE (SSE) ---
            res.setHeader('Content-Type', 'text/event-stream');
            res.setHeader('Cache-Control', 'no-cache');
            res.setHeader('Connection', 'keep-alive');

            const streamResponse = await meta.prompt(prompt, true, isNewConversation);

            let previousText = ""; // Biáº¿n Ä‘á»ƒ theo dÃµi text cÅ© nháº±m tÃ­nh delta

            for await (const chunk of streamResponse) {
                const fullText = chunk.message || "";
                
                // TÃ­nh toÃ¡n delta (pháº§n má»›i thÃªm vÃ o)
                // Meta AI tráº£ vá» full text tÃ­ch lÅ©y, OpenAI cáº§n delta
                let delta = "";
                if (fullText.startsWith(previousText)) {
                    delta = fullText.slice(previousText.length);
                } else {
                    // TrÆ°á»ng há»£p hiáº¿m: text bá»‹ thay Ä‘á»•i cáº¥u trÃºc, gá»­i luÃ´n full text má»›i
                    delta = fullText;
                }
                
                previousText = fullText;

                if (delta) {
                    const openaiChunk = {
                        id: `chatcmpl-${uuidv4()}`,
                        object: "chat.completion.chunk",
                        created: Math.floor(Date.now() / 1000),
                        model: model || "meta-llama-3",
                        choices: [{
                            index: 0,
                            delta: { content: delta },
                            finish_reason: null
                        }]
                    };
                    res.write(`data: ${JSON.stringify(openaiChunk)}\n\n`);
                }
            }

            // Káº¿t thÃºc stream
            res.write('data: [DONE]\n\n');
            res.end();
        }

    } catch (error) {
        console.error("Error processing request:", error);
        
        // Náº¿u lá»—i liÃªn quan Ä‘áº¿n session hoáº·c máº¡ng, reset instance Ä‘á»ƒ láº§n sau init láº¡i
        metaInstance = null;
        
        if (!res.headersSent) {
            res.status(500).json({
                error: {
                    message: error.message || "Internal Server Error",
                    type: "server_error",
                    code: 500
                }
            });
        } else {
            res.end();
        }
    }
});

// Start Server
app.listen(PORT, () => {
    console.log(`ğŸš€ OpenAI-compatible MetaAI server running on port ${PORT}`);
    console.log(`ğŸ”‘ API Key: ${SERVER_API_KEY}`);
});
