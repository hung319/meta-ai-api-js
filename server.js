require('dotenv').config();
const express = require('express');
const bodyParser = require('body-parser');
const cors = require('cors');
const { v4: uuidv4 } = require('uuid');
const MetaAI = require('./index');

const app = express();
const PORT = process.env.PORT || 3000;
const SERVER_API_KEY = process.env.API_KEY || "1";
const PROXY_URL = process.env.PROXY_URL || null;

// --- KHÃ”NG CÃ’N GLOBAL INSTANCE ---
// Má»—i request sáº½ tá»± táº¡o instance riÃªng biá»‡t

app.use(cors());
app.use(bodyParser.json());

// Auth Middleware
app.use((req, res, next) => {
    if (req.path === '/') return next();
    const authHeader = req.headers.authorization;
    const token = authHeader && authHeader.split(' ')[1];
    if (token !== SERVER_API_KEY) {
        return res.status(401).json({
            error: {
                message: "Invalid API Key",
                type: "invalid_request_error",
                param: null,
                code: "invalid_api_key"
            }
        });
    }
    next();
});

function convertMessagesToPrompt(messages) {
    const lastUserMessage = messages.slice().reverse().find(m => m.role === 'user');
    return lastUserMessage ? lastUserMessage.content : "Hello";
}

app.get('/', (req, res) => {
    res.send('Meta AI Service (Multi-Instance Mode) is running.');
});

app.get('/v1/models', (req, res) => {
    res.json({
        object: "list",
        data: [{
            id: "meta-llama-3",
            object: "model",
            created: 1677610602,
            owned_by: "meta-ai-wrapper"
        }]
    });
});

app.post('/v1/chat/completions', async (req, res) => {
    const { messages, stream = false, model } = req.body;

    if (!messages || !Array.isArray(messages)) {
        return res.status(400).json({ error: "Messages array required" });
    }

    // Biáº¿n instance cá»¥c bá»™, chá»‰ sá»‘ng trong 1 request nÃ y
    let meta = null;

    try {
        // 1. Kich hoat Multi-Instance: Táº¡o má»›i má»—i láº§n gá»i
        // console.log('ðŸ”„ Creating new MetaAI session for request...');
        meta = await MetaAI.create(null, null, PROXY_URL);
        
        const prompt = convertMessagesToPrompt(messages);
        
        // LuÃ´n táº¡o há»™i thoáº¡i má»›i Ä‘á»ƒ trÃ¡nh lá»—i signatures cÅ©
        const isNewConversation = true; 

        if (!stream) {
            // --- NON-STREAMING ---
            const response = await meta.prompt(prompt, false, isNewConversation);
            
            res.json({
                id: `chatcmpl-${uuidv4()}`,
                object: "chat.completion",
                created: Math.floor(Date.now() / 1000),
                model: model || "meta-llama-3",
                choices: [{
                    index: 0,
                    message: {
                        role: "assistant",
                        content: response.message,
                    },
                    finish_reason: "stop"
                }],
                usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }
            });

        } else {
            // --- STREAMING ---
            res.setHeader('Content-Type', 'text/event-stream');
            res.setHeader('Cache-Control', 'no-cache');
            res.setHeader('Connection', 'keep-alive');

            const streamResponse = await meta.prompt(prompt, true, isNewConversation);

            for await (const chunk of streamResponse) {
                // 2. Bá» Logic Delta: Gá»­i trá»±c tiáº¿p chunk nháº­n Ä‘Æ°á»£c
                // LÆ°u Ã½: Náº¿u src/main.js tráº£ vá» full text, client sáº½ bá»‹ láº·p chá»¯.
                // Náº¿u src/main.js Ä‘Ã£ xá»­ lÃ½ delta, thÃ¬ Ä‘oáº¡n nÃ y hoáº¡t Ä‘á»™ng Ä‘Ãºng.
                const content = chunk.message; 

                if (content) {
                    const openaiChunk = {
                        id: `chatcmpl-${uuidv4()}`,
                        object: "chat.completion.chunk",
                        created: Math.floor(Date.now() / 1000),
                        model: model || "meta-llama-3",
                        choices: [{
                            index: 0,
                            delta: { content: content },
                            finish_reason: null
                        }]
                    };
                    res.write(`data: ${JSON.stringify(openaiChunk)}\n\n`);
                }
            }

            res.write('data: [DONE]\n\n');
            res.end();
        }

    } catch (error) {
        console.error("Request Error:", error.message);
        if (!res.headersSent) {
            res.status(500).json({ error: "Internal Server Error" });
        } else {
            res.end();
        }
    } finally {
        // Dá»n dáº¹p memory náº¿u cáº§n (NodeJS tá»± GC, nhÆ°ng logic nÃ y clear ref)
        meta = null;
    }
});

app.listen(PORT, () => {
    console.log(`ðŸš€ Server (Multi-Instance) running on port ${PORT}`);
});
